{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOWdaKzElqzFYJdu6+gxST/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"W2vZgZLbRrqr","executionInfo":{"status":"ok","timestamp":1728490104583,"user_tz":-180,"elapsed":3667,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["# open text file and read in data as `text`\n","with open('dostoevsky.txt', 'r') as f:\n"," text = f.read()\n"," print(text[:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jFmQ554HR2jp","executionInfo":{"status":"ok","timestamp":1728490104583,"user_tz":-180,"elapsed":34,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}},"outputId":"882c8fc4-6396-46f2-adb1-dcc2b6801c20"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Федор Михайлович Достоевский\n","Бедные люди\n","Ох уж эти мне сказочники! Нет чтобы написать что-нибудь пол\n"]}]},{"cell_type":"markdown","source":["токенизация (кодирование символов числами)"],"metadata":{"id":"NzMApqnuSA7Q"}},{"cell_type":"code","source":["# encode the text and map each character to an integer and vice versa\n","# we create two dictionaries:\n","# 1. int2char, which maps integers to characters\n","# 2. char2int, which maps characters to unique integers\n","chars = tuple(set(text))\n","int2char = dict(enumerate(chars))\n","char2int = {ch: ii for ii, ch in int2char.items()}\n","# encode the text\n","encoded = np.array([char2int[ch] for ch in text])\n","print(encoded[:100])"],"metadata":{"id":"L-VMS5UmR6pI","executionInfo":{"status":"ok","timestamp":1728490104584,"user_tz":-180,"elapsed":30,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}},"outputId":"a3463e22-9496-4565-ba93-f662a3f7c1a5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[104 112 100  89  32  43  83  74 114  26  12  14  89   8  74  16  43 115\n","  89  51 120  89 112   8  51  22  74  12  54 103 112 100  11 128 112  43\n","  14 116 100  74  54  39 114  43  69  82  43  62 120  74  43  79  11 112\n","  43  51  22  26  88  89  16  11  74  22  74  98  43  45 112 120  43  16\n"," 120  89  59 128  43  11  26  23  74  51  26 120  97  43  16 120  89  68\n","  11  74  59  69 100  97  43  23  89  14]\n"]}]},{"cell_type":"markdown","source":["Предобработка данных (one-hot кодирование)"],"metadata":{"id":"EFpuDU9hcbJy"}},{"cell_type":"code","source":["def one_hot_encode(arr, n_labels):\n","\n"," # Initialize the the encoded array\n"," one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n","\n"," # Fill the appropriate elements with ones\n"," one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n","\n"," # Finally reshape it to get back to the original array\n"," one_hot = one_hot.reshape((*arr.shape, n_labels))\n","\n"," return one_hot"],"metadata":{"id":"omvrtEBPcWnR","executionInfo":{"status":"ok","timestamp":1728490104584,"user_tz":-180,"elapsed":27,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# check that the function works as expected\n","test_seq = np.array([[3, 5, 1]])\n","one_hot = one_hot_encode(test_seq, 8)\n","print(one_hot)"],"metadata":{"id":"wfOU8g6MdM5T","executionInfo":{"status":"ok","timestamp":1728490104584,"user_tz":-180,"elapsed":26,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}},"outputId":"26d2979c-6a64-459d-d471-d05535e37f19","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0. 0.]\n","  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"]}]},{"cell_type":"code","source":["def get_batches(arr, batch_size, seq_length):\n"," '''Create a generator that returns batches of size\n"," batch_size x seq_length from arr.\n","\n"," Arguments\n"," ---------\n"," arr: Array you want to make batches from\n"," batch_size: Batch size, the number of sequences per batch\n"," seq_length: Number of encoded chars in a sequence\n"," '''\n","\n"," ## TODO: Get the number of batches we can make\n"," n_batches =len(arr)//(batch_size*seq_length)\n","\n"," ## TODO: Keep only enough characters to make full batches\n"," arr =arr[:n_batches*batch_size*seq_length]\n","\n"," ## TODO: Reshape into batch_size rows\n"," arr =arr.reshape([batch_size, -1])\n","\n"," ## Iterate over the batches using a window of size seq_length\n"," for n in range(0, arr.shape[1], seq_length):\n"," # The features\n","  x = arr[:, n:n+seq_length]\n"," # The targets, shifted by one\n","  y = np.zeros_like(x)\n","  try:\n","    y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n","  except IndexError:\n","    y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n","  yield x, y\n"],"metadata":{"id":"IGHNT4TVkVu0","executionInfo":{"status":"ok","timestamp":1728490104584,"user_tz":-180,"elapsed":22,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["batches = get_batches(encoded, 8, 50)\n","x, y = next(batches)\n","# printing out the first 10 items in a sequence\n","print('x\\n', x[:10, :10])\n","print('\\ny\\n', y[:10, :10])\n"],"metadata":{"id":"SYo28P54CwXa","executionInfo":{"status":"ok","timestamp":1728490104584,"user_tz":-180,"elapsed":21,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}},"outputId":"62202f49-2c22-4430-a179-98f85f537105","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["x\n"," [[104 112 100  89  32  43  83  74 114  26]\n"," [ 14  40  43  69  14 128  59  26  29  51]\n"," [ 43   8  43  23  32  74   8 128  16  11]\n"," [ 51  97  43  89  51 120  26   8  26 120]\n"," [100  97  43  11  26  79 112  32 112  11]\n"," [ 43  51 112  67  89 100  11  29  43 100]\n"," [ 11 112  43  51  89   8  51 112  79  43]\n"," [ 74  43  23  89  23  26  14  89  40  43]]\n","\n","y\n"," [[112 100  89  32  43  83  74 114  26  12]\n"," [ 40  43  69  14 128  59  26  29  51  97]\n"," [  8  43  23  32  74   8 128  16  11  89]\n"," [ 97  43  89  51 120  26   8  26 120  97]\n"," [ 97  43  11  26  79 112  32 112  11  74]\n"," [ 51 112  67  89 100  11  29  43 100  89]\n"," [112  43  51  89   8  51 112  79  43 112]\n"," [ 43  23  89  23  26  14  89  40  43  26]]\n"]}]},{"cell_type":"code","source":["# check if GPU is available\n","train_on_gpu = torch.cuda.is_available()\n","if(train_on_gpu):\n","  print('Training on GPU!')\n","else:\n","  print('No GPU available, training on CPU; consider making n_epochs very small.')\n","class CharRNN(nn.Module):\n","  def __init__(self, tokens, n_hidden=256, n_layers=3, drop_prob=0.5, lr=0.001):\n","    super().__init__()\n","    self.drop_prob = drop_prob\n","    self.n_layers = n_layers\n","    self.n_hidden = n_hidden\n","    self.lr = lr\n","\n","    # creating character dictionaries\n","    self.chars = tokens\n","    self.int2char = dict(enumerate(self.chars))\n","    self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n","\n","    ## TODO: define the layers of the model\n","    self.dropout = nn.Dropout(drop_prob);\n","\n","    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n","\n","    self.fc =nn.Linear(n_hidden, len(self.chars))\n","\n","\n","  def forward(self, x, hidden):\n","    # TODO: Get the outputs and the new hidden state from the lstm\n","    r_output, hidden = self.lstm(x, hidden)\n","\n","    ## TODO: pass through a dropout layer\n","    out =self.dropout(r_output)\n","\n","    #Stack up LSTM output using view\n","    #you may need to use contiguous to reshape the output\n","    out =out.contiguous().view(-1, self.n_hidden)\n","\n","    ## TODO: put x through the fully-connected layer\n","    out = self.fc(out)\n","\n","    # return the final output and the hidden state\n","    return out, hidden\n","  def init_hidden(self, batch_size):\n","    ''' Initializes hidden state '''\n","    # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n","    # initialized to zero, for hidden state and cell state of LSTM\n","    weight = next(self.parameters()).data\n","\n","    if (train_on_gpu):\n","        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n","    else:\n","        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n","\n","    return hidden\n","\n","\n"],"metadata":{"id":"XBAdotfo59o7","executionInfo":{"status":"ok","timestamp":1728490105472,"user_tz":-180,"elapsed":905,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}},"outputId":"6795d252-6288-45e0-d4d4-8e078dcf0072","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on GPU!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"tU_t2DBXdTRT","executionInfo":{"status":"ok","timestamp":1728490105473,"user_tz":-180,"elapsed":900,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Тренировка"],"metadata":{"id":"ng6RCW4RB11Q"}},{"cell_type":"code","source":["def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n","    ''' Training a network\n","\n","        Arguments\n","        ---------\n","\n","        net: CharRNN network\n","        data: text data to train the network\n","        epochs: Number of epochs to train\n","        batch_size: Number of mini-sequences per mini-batch, aka batch size\n","        seq_length: Number of character steps per mini-batch\n","        lr: learning rate\n","        clip: gradient clipping\n","        val_frac: Fraction of data to hold out for validation\n","        print_every: Number of steps for printing training and validation loss\n","\n","    '''\n","    net.train()\n","\n","    opt = torch.optim.Adam(net.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # create training and validation data\n","    val_idx = int(len(data)*(1-val_frac))\n","    data, val_data = data[:val_idx], data[val_idx:]\n","\n","    if(train_on_gpu):\n","        net.cuda()\n","\n","    counter = 0\n","    n_chars = len(net.chars)\n","    for e in range(epochs):\n","        # initialize hidden state\n","        h = net.init_hidden(batch_size)\n","\n","        for x, y in get_batches(data, batch_size, seq_length):\n","            counter += 1\n","\n","            # One-hot encode our data and make them Torch tensors\n","            x = one_hot_encode(x, n_chars)\n","            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n","\n","            if(train_on_gpu):\n","                inputs, targets = inputs.cuda(), targets.cuda()\n","\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            h = tuple([each.data for each in h])\n","\n","            # zero accumulated gradients\n","            net.zero_grad()\n","\n","            # get the output from the model\n","            output, h = net(inputs, h)\n","\n","            # calculate the loss and perform backprop\n","            loss = criterion(output, targets.view(batch_size*seq_length))\n","            loss.backward()\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(net.parameters(), clip)\n","            opt.step()\n","\n","            # loss stats\n","            if counter % print_every == 0:\n","                # Get validation loss\n","                val_h = net.init_hidden(batch_size)\n","                val_losses = []\n","                net.eval()\n","                for x, y in get_batches(val_data, batch_size, seq_length):\n","                    # One-hot encode our data and make them Torch tensors\n","                    x = one_hot_encode(x, n_chars)\n","                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n","\n","                    # Creating new variables for the hidden state, otherwise\n","                    # we'd backprop through the entire training history\n","                    val_h = tuple([each.data for each in val_h])\n","\n","                    inputs, targets = x, y\n","                    if(train_on_gpu):\n","                        inputs, targets = inputs.cuda(), targets.cuda()\n","\n","                    output, val_h = net(inputs, val_h)\n","                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n","\n","                    val_losses.append(val_loss.item())\n","\n","                net.train() # reset to train mode after iterationg through validation data\n","\n","                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                      \"Step: {}...\".format(counter),\n","                      \"Loss: {:.4f}...\".format(loss.item()),\n","                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"],"metadata":{"id":"htxkqQEERcU8","executionInfo":{"status":"ok","timestamp":1728490105473,"user_tz":-180,"elapsed":9,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Обучение"],"metadata":{"id":"wdEROgCbaBOR"}},{"cell_type":"code","source":["# define and print the net\n","n_hidden=512\n","n_layers=3\n","\n","net = CharRNN(chars, n_hidden, n_layers)\n","print(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3-1GCfXRVRT","executionInfo":{"status":"ok","timestamp":1728490105474,"user_tz":-180,"elapsed":9,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}},"outputId":"1a4bd64c-adc0-435f-b019-224d04fde182"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["CharRNN(\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (lstm): LSTM(133, 512, num_layers=3, batch_first=True, dropout=0.5)\n","  (fc): Linear(in_features=512, out_features=133, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["batch_size = 64\n","seq_length = 100 #max length verses\n","n_epochs = 50 # start smaller if you are just testing initial behavior\n","\n","# train the model\n","train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8H_CMkp6aLjx","executionInfo":{"status":"ok","timestamp":1728490408313,"user_tz":-180,"elapsed":302844,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}},"outputId":"431a4f5f-3d6c-4130-ab8a-a307b3669fe2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/50... Step: 10... Loss: 3.3643... Val Loss: 3.3240\n","Epoch: 1/50... Step: 20... Loss: 3.3259... Val Loss: 3.2938\n","Epoch: 1/50... Step: 30... Loss: 3.2964... Val Loss: 3.2849\n","Epoch: 1/50... Step: 40... Loss: 3.2586... Val Loss: 3.2850\n","Epoch: 1/50... Step: 50... Loss: 3.2754... Val Loss: 3.2793\n","Epoch: 1/50... Step: 60... Loss: 3.2276... Val Loss: 3.2816\n","Epoch: 2/50... Step: 70... Loss: 3.2366... Val Loss: 3.2840\n","Epoch: 2/50... Step: 80... Loss: 3.2335... Val Loss: 3.2813\n","Epoch: 2/50... Step: 90... Loss: 3.2276... Val Loss: 3.2849\n","Epoch: 2/50... Step: 100... Loss: 3.2349... Val Loss: 3.2850\n","Epoch: 2/50... Step: 110... Loss: 3.2284... Val Loss: 3.2838\n","Epoch: 2/50... Step: 120... Loss: 3.2378... Val Loss: 3.2807\n","Epoch: 3/50... Step: 130... Loss: 3.2231... Val Loss: 3.2824\n","Epoch: 3/50... Step: 140... Loss: 3.2148... Val Loss: 3.2729\n","Epoch: 3/50... Step: 150... Loss: 3.2205... Val Loss: 3.2457\n","Epoch: 3/50... Step: 160... Loss: 3.1236... Val Loss: 3.2049\n","Epoch: 3/50... Step: 170... Loss: 3.0219... Val Loss: 3.0787\n","Epoch: 3/50... Step: 180... Loss: 2.9133... Val Loss: 2.9884\n","Epoch: 4/50... Step: 190... Loss: 2.8693... Val Loss: 2.9251\n","Epoch: 4/50... Step: 200... Loss: 2.7699... Val Loss: 2.8638\n","Epoch: 4/50... Step: 210... Loss: 2.7833... Val Loss: 2.8138\n","Epoch: 4/50... Step: 220... Loss: 2.6634... Val Loss: 2.7823\n","Epoch: 4/50... Step: 230... Loss: 2.6537... Val Loss: 2.7483\n","Epoch: 4/50... Step: 240... Loss: 2.6105... Val Loss: 2.7236\n","Epoch: 5/50... Step: 250... Loss: 2.5825... Val Loss: 2.7197\n","Epoch: 5/50... Step: 260... Loss: 2.5795... Val Loss: 2.7021\n","Epoch: 5/50... Step: 270... Loss: 2.5558... Val Loss: 2.6753\n","Epoch: 5/50... Step: 280... Loss: 2.5151... Val Loss: 2.6624\n","Epoch: 5/50... Step: 290... Loss: 2.5194... Val Loss: 2.6414\n","Epoch: 5/50... Step: 300... Loss: 2.4993... Val Loss: 2.6271\n","Epoch: 6/50... Step: 310... Loss: 2.4500... Val Loss: 2.6092\n","Epoch: 6/50... Step: 320... Loss: 2.4745... Val Loss: 2.5960\n","Epoch: 6/50... Step: 330... Loss: 2.4423... Val Loss: 2.5819\n","Epoch: 6/50... Step: 340... Loss: 2.3899... Val Loss: 2.5758\n","Epoch: 6/50... Step: 350... Loss: 2.4274... Val Loss: 2.5527\n","Epoch: 6/50... Step: 360... Loss: 2.4013... Val Loss: 2.5351\n","Epoch: 7/50... Step: 370... Loss: 2.3752... Val Loss: 2.5323\n","Epoch: 7/50... Step: 380... Loss: 2.3593... Val Loss: 2.5174\n","Epoch: 7/50... Step: 390... Loss: 2.3769... Val Loss: 2.5035\n","Epoch: 7/50... Step: 400... Loss: 2.3179... Val Loss: 2.4971\n","Epoch: 7/50... Step: 410... Loss: 2.3231... Val Loss: 2.4809\n","Epoch: 7/50... Step: 420... Loss: 2.3295... Val Loss: 2.4600\n","Epoch: 8/50... Step: 430... Loss: 2.2816... Val Loss: 2.4626\n","Epoch: 8/50... Step: 440... Loss: 2.2723... Val Loss: 2.4445\n","Epoch: 8/50... Step: 450... Loss: 2.2659... Val Loss: 2.4334\n","Epoch: 8/50... Step: 460... Loss: 2.2704... Val Loss: 2.4183\n","Epoch: 8/50... Step: 470... Loss: 2.2429... Val Loss: 2.4120\n","Epoch: 8/50... Step: 480... Loss: 2.2489... Val Loss: 2.3984\n","Epoch: 9/50... Step: 490... Loss: 2.2252... Val Loss: 2.3849\n","Epoch: 9/50... Step: 500... Loss: 2.2289... Val Loss: 2.3761\n","Epoch: 9/50... Step: 510... Loss: 2.1979... Val Loss: 2.3683\n","Epoch: 9/50... Step: 520... Loss: 2.2328... Val Loss: 2.3558\n","Epoch: 9/50... Step: 530... Loss: 2.2062... Val Loss: 2.3457\n","Epoch: 9/50... Step: 540... Loss: 2.1960... Val Loss: 2.3359\n","Epoch: 10/50... Step: 550... Loss: 2.1710... Val Loss: 2.3202\n","Epoch: 10/50... Step: 560... Loss: 2.1927... Val Loss: 2.3116\n","Epoch: 10/50... Step: 570... Loss: 2.1432... Val Loss: 2.2997\n","Epoch: 10/50... Step: 580... Loss: 2.1259... Val Loss: 2.2923\n","Epoch: 10/50... Step: 590... Loss: 2.1192... Val Loss: 2.2762\n","Epoch: 10/50... Step: 600... Loss: 2.1294... Val Loss: 2.2625\n","Epoch: 10/50... Step: 610... Loss: 2.1194... Val Loss: 2.2505\n","Epoch: 11/50... Step: 620... Loss: 2.1457... Val Loss: 2.2496\n","Epoch: 11/50... Step: 630... Loss: 2.1362... Val Loss: 2.2378\n","Epoch: 11/50... Step: 640... Loss: 2.0294... Val Loss: 2.2260\n","Epoch: 11/50... Step: 650... Loss: 2.0661... Val Loss: 2.2218\n","Epoch: 11/50... Step: 660... Loss: 2.0814... Val Loss: 2.2025\n","Epoch: 11/50... Step: 670... Loss: 2.0187... Val Loss: 2.1934\n","Epoch: 12/50... Step: 680... Loss: 2.0295... Val Loss: 2.1873\n","Epoch: 12/50... Step: 690... Loss: 2.0356... Val Loss: 2.1794\n","Epoch: 12/50... Step: 700... Loss: 2.0019... Val Loss: 2.1720\n","Epoch: 12/50... Step: 710... Loss: 2.0018... Val Loss: 2.1605\n","Epoch: 12/50... Step: 720... Loss: 2.0050... Val Loss: 2.1504\n","Epoch: 12/50... Step: 730... Loss: 2.0048... Val Loss: 2.1459\n","Epoch: 13/50... Step: 740... Loss: 1.9702... Val Loss: 2.1353\n","Epoch: 13/50... Step: 750... Loss: 1.9650... Val Loss: 2.1285\n","Epoch: 13/50... Step: 760... Loss: 1.9660... Val Loss: 2.1178\n","Epoch: 13/50... Step: 770... Loss: 1.9634... Val Loss: 2.1129\n","Epoch: 13/50... Step: 780... Loss: 1.9255... Val Loss: 2.0937\n","Epoch: 13/50... Step: 790... Loss: 1.9554... Val Loss: 2.0905\n","Epoch: 14/50... Step: 800... Loss: 1.9792... Val Loss: 2.0876\n","Epoch: 14/50... Step: 810... Loss: 1.9080... Val Loss: 2.0834\n","Epoch: 14/50... Step: 820... Loss: 1.9560... Val Loss: 2.0706\n","Epoch: 14/50... Step: 830... Loss: 1.9297... Val Loss: 2.0654\n","Epoch: 14/50... Step: 840... Loss: 1.9158... Val Loss: 2.0545\n","Epoch: 14/50... Step: 850... Loss: 1.9080... Val Loss: 2.0468\n","Epoch: 15/50... Step: 860... Loss: 1.8796... Val Loss: 2.0490\n","Epoch: 15/50... Step: 870... Loss: 1.8834... Val Loss: 2.0425\n","Epoch: 15/50... Step: 880... Loss: 1.8715... Val Loss: 2.0350\n","Epoch: 15/50... Step: 890... Loss: 1.8457... Val Loss: 2.0295\n","Epoch: 15/50... Step: 900... Loss: 1.8872... Val Loss: 2.0155\n","Epoch: 15/50... Step: 910... Loss: 1.8350... Val Loss: 2.0045\n","Epoch: 16/50... Step: 920... Loss: 1.8407... Val Loss: 2.0123\n","Epoch: 16/50... Step: 930... Loss: 1.8484... Val Loss: 2.0100\n","Epoch: 16/50... Step: 940... Loss: 1.7956... Val Loss: 1.9951\n","Epoch: 16/50... Step: 950... Loss: 1.7728... Val Loss: 1.9914\n","Epoch: 16/50... Step: 960... Loss: 1.8312... Val Loss: 1.9816\n","Epoch: 16/50... Step: 970... Loss: 1.7950... Val Loss: 1.9766\n","Epoch: 17/50... Step: 980... Loss: 1.8316... Val Loss: 1.9746\n","Epoch: 17/50... Step: 990... Loss: 1.7875... Val Loss: 1.9721\n","Epoch: 17/50... Step: 1000... Loss: 1.7980... Val Loss: 1.9693\n","Epoch: 17/50... Step: 1010... Loss: 1.8073... Val Loss: 1.9574\n","Epoch: 17/50... Step: 1020... Loss: 1.7928... Val Loss: 1.9524\n","Epoch: 17/50... Step: 1030... Loss: 1.7983... Val Loss: 1.9532\n","Epoch: 18/50... Step: 1040... Loss: 1.7737... Val Loss: 1.9441\n","Epoch: 18/50... Step: 1050... Loss: 1.7893... Val Loss: 1.9442\n","Epoch: 18/50... Step: 1060... Loss: 1.7616... Val Loss: 1.9512\n","Epoch: 18/50... Step: 1070... Loss: 1.7860... Val Loss: 1.9408\n","Epoch: 18/50... Step: 1080... Loss: 1.7563... Val Loss: 1.9268\n","Epoch: 18/50... Step: 1090... Loss: 1.7316... Val Loss: 1.9268\n","Epoch: 19/50... Step: 1100... Loss: 1.7554... Val Loss: 1.9214\n","Epoch: 19/50... Step: 1110... Loss: 1.7524... Val Loss: 1.9171\n","Epoch: 19/50... Step: 1120... Loss: 1.7138... Val Loss: 1.9200\n","Epoch: 19/50... Step: 1130... Loss: 1.7752... Val Loss: 1.9146\n","Epoch: 19/50... Step: 1140... Loss: 1.7256... Val Loss: 1.9149\n","Epoch: 19/50... Step: 1150... Loss: 1.7459... Val Loss: 1.9009\n","Epoch: 20/50... Step: 1160... Loss: 1.7541... Val Loss: 1.9027\n","Epoch: 20/50... Step: 1170... Loss: 1.7759... Val Loss: 1.8923\n","Epoch: 20/50... Step: 1180... Loss: 1.6905... Val Loss: 1.8959\n","Epoch: 20/50... Step: 1190... Loss: 1.6759... Val Loss: 1.9007\n","Epoch: 20/50... Step: 1200... Loss: 1.7076... Val Loss: 1.8929\n","Epoch: 20/50... Step: 1210... Loss: 1.7344... Val Loss: 1.8833\n","Epoch: 20/50... Step: 1220... Loss: 1.7275... Val Loss: 1.8888\n","Epoch: 21/50... Step: 1230... Loss: 1.7547... Val Loss: 1.8812\n","Epoch: 21/50... Step: 1240... Loss: 1.7347... Val Loss: 1.8816\n","Epoch: 21/50... Step: 1250... Loss: 1.6332... Val Loss: 1.8811\n","Epoch: 21/50... Step: 1260... Loss: 1.7075... Val Loss: 1.8747\n","Epoch: 21/50... Step: 1270... Loss: 1.7185... Val Loss: 1.8707\n","Epoch: 21/50... Step: 1280... Loss: 1.6636... Val Loss: 1.8663\n","Epoch: 22/50... Step: 1290... Loss: 1.6734... Val Loss: 1.8721\n","Epoch: 22/50... Step: 1300... Loss: 1.6813... Val Loss: 1.8616\n","Epoch: 22/50... Step: 1310... Loss: 1.6670... Val Loss: 1.8595\n","Epoch: 22/50... Step: 1320... Loss: 1.6721... Val Loss: 1.8577\n","Epoch: 22/50... Step: 1330... Loss: 1.6831... Val Loss: 1.8564\n","Epoch: 22/50... Step: 1340... Loss: 1.6602... Val Loss: 1.8518\n","Epoch: 23/50... Step: 1350... Loss: 1.6545... Val Loss: 1.8502\n","Epoch: 23/50... Step: 1360... Loss: 1.6510... Val Loss: 1.8544\n","Epoch: 23/50... Step: 1370... Loss: 1.6606... Val Loss: 1.8453\n","Epoch: 23/50... Step: 1380... Loss: 1.6396... Val Loss: 1.8512\n","Epoch: 23/50... Step: 1390... Loss: 1.6405... Val Loss: 1.8400\n","Epoch: 23/50... Step: 1400... Loss: 1.6579... Val Loss: 1.8452\n","Epoch: 24/50... Step: 1410... Loss: 1.6636... Val Loss: 1.8387\n","Epoch: 24/50... Step: 1420... Loss: 1.6101... Val Loss: 1.8384\n","Epoch: 24/50... Step: 1430... Loss: 1.6391... Val Loss: 1.8352\n","Epoch: 24/50... Step: 1440... Loss: 1.6735... Val Loss: 1.8372\n","Epoch: 24/50... Step: 1450... Loss: 1.6583... Val Loss: 1.8307\n","Epoch: 24/50... Step: 1460... Loss: 1.6407... Val Loss: 1.8234\n","Epoch: 25/50... Step: 1470... Loss: 1.6043... Val Loss: 1.8285\n","Epoch: 25/50... Step: 1480... Loss: 1.6059... Val Loss: 1.8256\n","Epoch: 25/50... Step: 1490... Loss: 1.5989... Val Loss: 1.8276\n","Epoch: 25/50... Step: 1500... Loss: 1.6058... Val Loss: 1.8216\n","Epoch: 25/50... Step: 1510... Loss: 1.6228... Val Loss: 1.8230\n","Epoch: 25/50... Step: 1520... Loss: 1.5694... Val Loss: 1.8111\n","Epoch: 26/50... Step: 1530... Loss: 1.5854... Val Loss: 1.8147\n","Epoch: 26/50... Step: 1540... Loss: 1.5884... Val Loss: 1.8214\n","Epoch: 26/50... Step: 1550... Loss: 1.5607... Val Loss: 1.8190\n","Epoch: 26/50... Step: 1560... Loss: 1.5411... Val Loss: 1.8094\n","Epoch: 26/50... Step: 1570... Loss: 1.5905... Val Loss: 1.8079\n","Epoch: 26/50... Step: 1580... Loss: 1.5539... Val Loss: 1.8123\n","Epoch: 27/50... Step: 1590... Loss: 1.6089... Val Loss: 1.8059\n","Epoch: 27/50... Step: 1600... Loss: 1.5579... Val Loss: 1.8040\n","Epoch: 27/50... Step: 1610... Loss: 1.5697... Val Loss: 1.8068\n","Epoch: 27/50... Step: 1620... Loss: 1.5730... Val Loss: 1.8078\n","Epoch: 27/50... Step: 1630... Loss: 1.5766... Val Loss: 1.8000\n","Epoch: 27/50... Step: 1640... Loss: 1.5863... Val Loss: 1.8007\n","Epoch: 28/50... Step: 1650... Loss: 1.5488... Val Loss: 1.8019\n","Epoch: 28/50... Step: 1660... Loss: 1.5583... Val Loss: 1.7970\n","Epoch: 28/50... Step: 1670... Loss: 1.5540... Val Loss: 1.7981\n","Epoch: 28/50... Step: 1680... Loss: 1.5621... Val Loss: 1.8088\n","Epoch: 28/50... Step: 1690... Loss: 1.5599... Val Loss: 1.8013\n","Epoch: 28/50... Step: 1700... Loss: 1.5273... Val Loss: 1.7932\n","Epoch: 29/50... Step: 1710... Loss: 1.5440... Val Loss: 1.7998\n","Epoch: 29/50... Step: 1720... Loss: 1.5647... Val Loss: 1.7985\n","Epoch: 29/50... Step: 1730... Loss: 1.5115... Val Loss: 1.7921\n","Epoch: 29/50... Step: 1740... Loss: 1.5726... Val Loss: 1.7898\n","Epoch: 29/50... Step: 1750... Loss: 1.5556... Val Loss: 1.7966\n","Epoch: 29/50... Step: 1760... Loss: 1.5496... Val Loss: 1.7882\n","Epoch: 30/50... Step: 1770... Loss: 1.5717... Val Loss: 1.7976\n","Epoch: 30/50... Step: 1780... Loss: 1.5851... Val Loss: 1.7931\n","Epoch: 30/50... Step: 1790... Loss: 1.4913... Val Loss: 1.7883\n","Epoch: 30/50... Step: 1800... Loss: 1.5079... Val Loss: 1.7849\n","Epoch: 30/50... Step: 1810... Loss: 1.5235... Val Loss: 1.7917\n","Epoch: 30/50... Step: 1820... Loss: 1.5461... Val Loss: 1.7870\n","Epoch: 30/50... Step: 1830... Loss: 1.5597... Val Loss: 1.7830\n","Epoch: 31/50... Step: 1840... Loss: 1.5721... Val Loss: 1.7872\n","Epoch: 31/50... Step: 1850... Loss: 1.5619... Val Loss: 1.7921\n","Epoch: 31/50... Step: 1860... Loss: 1.4591... Val Loss: 1.7826\n","Epoch: 31/50... Step: 1870... Loss: 1.5360... Val Loss: 1.7794\n","Epoch: 31/50... Step: 1880... Loss: 1.5341... Val Loss: 1.7754\n","Epoch: 31/50... Step: 1890... Loss: 1.5025... Val Loss: 1.7844\n","Epoch: 32/50... Step: 1900... Loss: 1.5112... Val Loss: 1.7850\n","Epoch: 32/50... Step: 1910... Loss: 1.5211... Val Loss: 1.7807\n","Epoch: 32/50... Step: 1920... Loss: 1.5155... Val Loss: 1.7789\n","Epoch: 32/50... Step: 1930... Loss: 1.5264... Val Loss: 1.7770\n","Epoch: 32/50... Step: 1940... Loss: 1.5152... Val Loss: 1.7778\n","Epoch: 32/50... Step: 1950... Loss: 1.5015... Val Loss: 1.7798\n","Epoch: 33/50... Step: 1960... Loss: 1.5352... Val Loss: 1.7784\n","Epoch: 33/50... Step: 1970... Loss: 1.4898... Val Loss: 1.7809\n","Epoch: 33/50... Step: 1980... Loss: 1.5138... Val Loss: 1.7790\n","Epoch: 33/50... Step: 1990... Loss: 1.4950... Val Loss: 1.7812\n","Epoch: 33/50... Step: 2000... Loss: 1.4829... Val Loss: 1.7811\n","Epoch: 33/50... Step: 2010... Loss: 1.5064... Val Loss: 1.7722\n","Epoch: 34/50... Step: 2020... Loss: 1.5227... Val Loss: 1.7746\n","Epoch: 34/50... Step: 2030... Loss: 1.4803... Val Loss: 1.7728\n","Epoch: 34/50... Step: 2040... Loss: 1.4775... Val Loss: 1.7748\n","Epoch: 34/50... Step: 2050... Loss: 1.5193... Val Loss: 1.7706\n","Epoch: 34/50... Step: 2060... Loss: 1.5038... Val Loss: 1.7745\n","Epoch: 34/50... Step: 2070... Loss: 1.4932... Val Loss: 1.7676\n","Epoch: 35/50... Step: 2080... Loss: 1.4740... Val Loss: 1.7745\n","Epoch: 35/50... Step: 2090... Loss: 1.4716... Val Loss: 1.7713\n","Epoch: 35/50... Step: 2100... Loss: 1.4603... Val Loss: 1.7713\n","Epoch: 35/50... Step: 2110... Loss: 1.4656... Val Loss: 1.7739\n","Epoch: 35/50... Step: 2120... Loss: 1.4756... Val Loss: 1.7721\n","Epoch: 35/50... Step: 2130... Loss: 1.4461... Val Loss: 1.7684\n","Epoch: 36/50... Step: 2140... Loss: 1.4625... Val Loss: 1.7709\n","Epoch: 36/50... Step: 2150... Loss: 1.4650... Val Loss: 1.7758\n","Epoch: 36/50... Step: 2160... Loss: 1.4324... Val Loss: 1.7694\n","Epoch: 36/50... Step: 2170... Loss: 1.4279... Val Loss: 1.7715\n","Epoch: 36/50... Step: 2180... Loss: 1.4603... Val Loss: 1.7720\n","Epoch: 36/50... Step: 2190... Loss: 1.4496... Val Loss: 1.7606\n","Epoch: 37/50... Step: 2200... Loss: 1.4829... Val Loss: 1.7691\n","Epoch: 37/50... Step: 2210... Loss: 1.4290... Val Loss: 1.7810\n","Epoch: 37/50... Step: 2220... Loss: 1.4384... Val Loss: 1.7755\n","Epoch: 37/50... Step: 2230... Loss: 1.4498... Val Loss: 1.7773\n","Epoch: 37/50... Step: 2240... Loss: 1.4577... Val Loss: 1.7657\n","Epoch: 37/50... Step: 2250... Loss: 1.4544... Val Loss: 1.7578\n","Epoch: 38/50... Step: 2260... Loss: 1.4414... Val Loss: 1.7711\n","Epoch: 38/50... Step: 2270... Loss: 1.4474... Val Loss: 1.7803\n","Epoch: 38/50... Step: 2280... Loss: 1.4316... Val Loss: 1.7739\n","Epoch: 38/50... Step: 2290... Loss: 1.4578... Val Loss: 1.7800\n","Epoch: 38/50... Step: 2300... Loss: 1.4394... Val Loss: 1.7675\n","Epoch: 38/50... Step: 2310... Loss: 1.4111... Val Loss: 1.7688\n","Epoch: 39/50... Step: 2320... Loss: 1.4270... Val Loss: 1.7811\n","Epoch: 39/50... Step: 2330... Loss: 1.4331... Val Loss: 1.7846\n","Epoch: 39/50... Step: 2340... Loss: 1.3908... Val Loss: 1.7759\n","Epoch: 39/50... Step: 2350... Loss: 1.4315... Val Loss: 1.7792\n","Epoch: 39/50... Step: 2360... Loss: 1.4300... Val Loss: 1.7772\n","Epoch: 39/50... Step: 2370... Loss: 1.4354... Val Loss: 1.7786\n","Epoch: 40/50... Step: 2380... Loss: 1.4578... Val Loss: 1.7872\n","Epoch: 40/50... Step: 2390... Loss: 1.4560... Val Loss: 1.7768\n","Epoch: 40/50... Step: 2400... Loss: 1.3890... Val Loss: 1.7710\n","Epoch: 40/50... Step: 2410... Loss: 1.4075... Val Loss: 1.7666\n","Epoch: 40/50... Step: 2420... Loss: 1.4235... Val Loss: 1.7760\n","Epoch: 40/50... Step: 2430... Loss: 1.4314... Val Loss: 1.7792\n","Epoch: 40/50... Step: 2440... Loss: 1.4525... Val Loss: 1.7705\n","Epoch: 41/50... Step: 2450... Loss: 1.4628... Val Loss: 1.7607\n","Epoch: 41/50... Step: 2460... Loss: 1.4442... Val Loss: 1.7725\n","Epoch: 41/50... Step: 2470... Loss: 1.3575... Val Loss: 1.7739\n","Epoch: 41/50... Step: 2480... Loss: 1.4158... Val Loss: 1.7779\n","Epoch: 41/50... Step: 2490... Loss: 1.4230... Val Loss: 1.7666\n","Epoch: 41/50... Step: 2500... Loss: 1.4020... Val Loss: 1.7593\n","Epoch: 42/50... Step: 2510... Loss: 1.4259... Val Loss: 1.7617\n","Epoch: 42/50... Step: 2520... Loss: 1.4215... Val Loss: 1.7807\n","Epoch: 42/50... Step: 2530... Loss: 1.4058... Val Loss: 1.7720\n","Epoch: 42/50... Step: 2540... Loss: 1.4255... Val Loss: 1.7759\n","Epoch: 42/50... Step: 2550... Loss: 1.4090... Val Loss: 1.7647\n","Epoch: 42/50... Step: 2560... Loss: 1.4009... Val Loss: 1.7660\n","Epoch: 43/50... Step: 2570... Loss: 1.4294... Val Loss: 1.7724\n","Epoch: 43/50... Step: 2580... Loss: 1.4104... Val Loss: 1.7823\n","Epoch: 43/50... Step: 2590... Loss: 1.4077... Val Loss: 1.7753\n","Epoch: 43/50... Step: 2600... Loss: 1.3988... Val Loss: 1.7802\n","Epoch: 43/50... Step: 2610... Loss: 1.3867... Val Loss: 1.7687\n","Epoch: 43/50... Step: 2620... Loss: 1.4079... Val Loss: 1.7670\n","Epoch: 44/50... Step: 2630... Loss: 1.3999... Val Loss: 1.7773\n","Epoch: 44/50... Step: 2640... Loss: 1.3835... Val Loss: 1.7881\n","Epoch: 44/50... Step: 2650... Loss: 1.3817... Val Loss: 1.7853\n","Epoch: 44/50... Step: 2660... Loss: 1.4184... Val Loss: 1.7847\n","Epoch: 44/50... Step: 2670... Loss: 1.4207... Val Loss: 1.7714\n","Epoch: 44/50... Step: 2680... Loss: 1.4028... Val Loss: 1.7742\n","Epoch: 45/50... Step: 2690... Loss: 1.3764... Val Loss: 1.7853\n","Epoch: 45/50... Step: 2700... Loss: 1.3770... Val Loss: 1.7816\n","Epoch: 45/50... Step: 2710... Loss: 1.3685... Val Loss: 1.7802\n","Epoch: 45/50... Step: 2720... Loss: 1.3809... Val Loss: 1.7865\n","Epoch: 45/50... Step: 2730... Loss: 1.4090... Val Loss: 1.7808\n","Epoch: 45/50... Step: 2740... Loss: 1.3617... Val Loss: 1.7906\n","Epoch: 46/50... Step: 2750... Loss: 1.3642... Val Loss: 1.7917\n","Epoch: 46/50... Step: 2760... Loss: 1.3709... Val Loss: 1.7982\n","Epoch: 46/50... Step: 2770... Loss: 1.3367... Val Loss: 1.7965\n","Epoch: 46/50... Step: 2780... Loss: 1.3298... Val Loss: 1.7914\n","Epoch: 46/50... Step: 2790... Loss: 1.3725... Val Loss: 1.7866\n","Epoch: 46/50... Step: 2800... Loss: 1.3432... Val Loss: 1.7862\n","Epoch: 47/50... Step: 2810... Loss: 1.3845... Val Loss: 1.7857\n","Epoch: 47/50... Step: 2820... Loss: 1.3385... Val Loss: 1.7879\n","Epoch: 47/50... Step: 2830... Loss: 1.3673... Val Loss: 1.7753\n","Epoch: 47/50... Step: 2840... Loss: 1.3625... Val Loss: 1.7799\n","Epoch: 47/50... Step: 2850... Loss: 1.3840... Val Loss: 1.7801\n","Epoch: 47/50... Step: 2860... Loss: 1.3726... Val Loss: 1.7812\n","Epoch: 48/50... Step: 2870... Loss: 1.3467... Val Loss: 1.7818\n","Epoch: 48/50... Step: 2880... Loss: 1.3684... Val Loss: 1.7871\n","Epoch: 48/50... Step: 2890... Loss: 1.3604... Val Loss: 1.7787\n","Epoch: 48/50... Step: 2900... Loss: 1.3601... Val Loss: 1.7790\n","Epoch: 48/50... Step: 2910... Loss: 1.3418... Val Loss: 1.7832\n","Epoch: 48/50... Step: 2920... Loss: 1.3242... Val Loss: 1.7803\n","Epoch: 49/50... Step: 2930... Loss: 1.3465... Val Loss: 1.7914\n","Epoch: 49/50... Step: 2940... Loss: 1.3512... Val Loss: 1.7911\n","Epoch: 49/50... Step: 2950... Loss: 1.3248... Val Loss: 1.7967\n","Epoch: 49/50... Step: 2960... Loss: 1.3551... Val Loss: 1.7966\n","Epoch: 49/50... Step: 2970... Loss: 1.3592... Val Loss: 1.7956\n","Epoch: 49/50... Step: 2980... Loss: 1.3551... Val Loss: 1.7903\n","Epoch: 50/50... Step: 2990... Loss: 1.3669... Val Loss: 1.7981\n","Epoch: 50/50... Step: 3000... Loss: 1.3776... Val Loss: 1.7949\n","Epoch: 50/50... Step: 3010... Loss: 1.2991... Val Loss: 1.8083\n","Epoch: 50/50... Step: 3020... Loss: 1.3183... Val Loss: 1.7940\n","Epoch: 50/50... Step: 3030... Loss: 1.3385... Val Loss: 1.7984\n","Epoch: 50/50... Step: 3040... Loss: 1.3501... Val Loss: 1.7946\n","Epoch: 50/50... Step: 3050... Loss: 1.3725... Val Loss: 1.8015\n"]}]},{"cell_type":"markdown","source":["Сохранение модели"],"metadata":{"id":"ghpRFh62bK4G"}},{"cell_type":"code","source":["model_name = 'rnn_x_epoch.net'\n","\n","checkpoint = {'n_hidden': net.n_hidden,\n","              'n_layers': net.n_layers,\n","              'state_dict': net.state_dict(),\n","              'tokens': net.chars}\n","\n","with open(model_name, 'wb') as f:\n","    torch.save(checkpoint, f)"],"metadata":{"id":"6zE3CKu5aOQI","executionInfo":{"status":"ok","timestamp":1728490408313,"user_tz":-180,"elapsed":36,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Предсказание. Сэмплирование."],"metadata":{"id":"s8QCbS8OcGo3"}},{"cell_type":"code","source":["def predict(net, char, h=None, top_k=None):\n","        ''' Given a character, predict the next character.\n","            Returns the predicted character and the hidden state.\n","        '''\n","\n","        # tensor inputs\n","        x = np.array([[net.char2int[char]]])\n","        x = one_hot_encode(x, len(net.chars))\n","        inputs = torch.from_numpy(x)\n","\n","        if(train_on_gpu):\n","            inputs = inputs.cuda()\n","\n","        # detach hidden state from history\n","        h = tuple([each.data for each in h])\n","        # get the output of the model\n","        out, h = net(inputs, h)\n","\n","        # get the character probabilities\n","        # apply softmax to get p probabilities for the likely next character giving x\n","        p = F.softmax(out, dim=1).data\n","        if(train_on_gpu):\n","            p = p.cpu() # move to cpu\n","\n","        # get top characters\n","        # considering the k most probable characters with topk method\n","        if top_k is None:\n","            top_ch = np.arange(len(net.chars))\n","        else:\n","            p, top_ch = p.topk(top_k)\n","            top_ch = top_ch.numpy().squeeze()\n","\n","        # select the likely next character with some element of randomness\n","        p = p.numpy().squeeze()\n","        char = np.random.choice(top_ch, p=p/p.sum())\n","\n","        # return the encoded value of the predicted char and the hidden state\n","        return net.int2char[char], h"],"metadata":{"id":"YVF_-s-8cKyv","executionInfo":{"status":"ok","timestamp":1728490408313,"user_tz":-180,"elapsed":11,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def sample(net, size, prime='The', top_k=None):\n","\n","    if(train_on_gpu):\n","        net.cuda()\n","    else:\n","        net.cpu()\n","\n","    net.eval() # eval mode\n","\n","    # First off, run through the prime characters\n","    chars = [ch for ch in prime]\n","    h = net.init_hidden(1)\n","    for ch in prime:\n","        char, h = predict(net, ch, h, top_k=top_k)\n","\n","    chars.append(char)\n","\n","    # Now pass in the previous character and get a new one\n","    for ii in range(size):\n","        char, h = predict(net, chars[-1], h, top_k=top_k)\n","        chars.append(char)\n","\n","    return ''.join(chars)"],"metadata":{"id":"CbvbEmEucPYW","executionInfo":{"status":"ok","timestamp":1728490408313,"user_tz":-180,"elapsed":10,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["print(sample(net, 1000, prime='Анна', top_k=5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"noypWQqFcXYg","executionInfo":{"status":"ok","timestamp":1728490408990,"user_tz":-180,"elapsed":687,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}},"outputId":"6c355687-a6ba-4a10-b7f9-a539eec6f150"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Анная, что вы сами не забыли его!.. Об этом не смотрите, что я не смею совсем свое рассказать.\n","Ваш их добрый, никогда ни старико не смел подомнил и, все такая женщена. Письмо переменятся, право, потому что он не могу ничего. Вставал вы мои совершенно в то же никогда на вестет, и перепомнились, как я все это будет, теперь подошно покупаете, а все так и трудно будет? Я не стала после, по крайней мере переменить свою сердце, а все это бы со мной не знали, все сказать, когда я сам совестно. Вы меня ни наконец он посыл и с ним под кухла на всякой случае. Наконец она не понимает, что-то сердце и все только потом. Я так же случалась, так что вот вот, не стел не спать, и в таком не знаете, что она проходили положинить, чтоб я не пришел под пред нами старика, не слыша было с совершенною наша иструтить свое видным душевной. Я последнюе весьма решенок вы свои дело на все письма, так ведь я подумал. Я начал меня в нет такой простой, подумайте, что он не продолжали свою руку и делал с ним и должности, т\n"]}]},{"cell_type":"markdown","source":["Загрузка модели"],"metadata":{"id":"RvXKpTRucdOf"}},{"cell_type":"code","source":["# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n","with open('rnn_x_epoch.net', 'rb') as f:\n","    checkpoint = torch.load(f)\n","\n","loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n","loaded.load_state_dict(checkpoint['state_dict'])\n","\n","# Sample using a loaded model\n","print(sample(loaded, 2000, top_k=5, prime=\"И он сказал\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TTnlWvP-cc6f","executionInfo":{"status":"ok","timestamp":1728490547080,"user_tz":-180,"elapsed":2104,"user":{"displayName":"Диана Пономарева","userId":"18444576338441786066"}},"outputId":"dbb823de-a9fe-4e55-b171-5122509cb4db"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-18-ad4007b597b5>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(f)\n"]},{"output_type":"stream","name":"stdout","text":["И он сказал; что он вышел в том! что он приходил меня по себя не могла, не предудательные, что у меня пред прочли, как же все стоять под досадного, что все это новые, как он в таком старенном совственнему, что вы не мог следы и темною просто время. Я ведь сам не смел на свои сердце, что вы напечатате? \n","— Ну, не сказал! \n","От манушки, нужно быть такая непременно письмо не могу станете! Нет, ведь только я не знаю, что я сам стоял с капмам. Я весь представилось ни в половеную себе свой привыкаешься, тужаться и собрались. Прислали прохожий не смел не забыла. Я вам ведь не подновила свои приступительно в комнату, и нет ни дума, то я все выражение не смысли, что он сердется. Но подумайте, чем мое слово бы не могу. Вы только посылаю в свою ветреное, то, что он не знаю, что я сама сердце представил предумательного ни с полениной придум на слова и сказал меня. Ну, что он только что становится на все время, как бы и всякий про себя, то все такие надостите меня с пред плочем ничего. Обедались; вы так сказать, что волнения не буду себя великому нужно. Ваш пришло, то вы, как будто не потому, что я сегодня, так вы теперь мне дело! А я все это посмотрела не слышал. Аннет; я на себя своих посолиме и все, что я по подругам сапога, только вы надебдается, чтобы не знаю, что оно всегда в стольке странное песебомные и сосет... \n","— Не воскрисать не могли в мее свое состоюное. Он со мной стала полесаться, который вы послушайтесь, да и все это пришлы и перед своего собы, так то есть я с неграмимениим в своем дурном образом, и сама всями стула и не предчавливому, как вы с васими были положимой совсем. Вот у меня встречиться в посылке. \n","— До самого все в портрет, на него слова. Всё становится все совершенно посторенного сердца. Он пришла к нам не слышать с какой-нибудь письмо стало быть, не знаю; но я почти не больше простой, не придумом, как будто все так не приходил в право, как будто неприменно сегодня, — прощайте, вам сказать, что он с концом. Не знаю, что я сам не знаю, что уж сказать, маточка, что он\n"]}]}]}